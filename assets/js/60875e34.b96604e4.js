"use strict";(self.webpackChunkkubedl_website=self.webpackChunkkubedl_website||[]).push([[4554],{3905:function(e,t,n){n.d(t,{Zo:function(){return m},kt:function(){return c}});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},l=Object.keys(e);for(o=0;o<l.length;o++)n=l[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(o=0;o<l.length;o++)n=l[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var d=o.createContext({}),s=function(e){var t=o.useContext(d),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},m=function(e){var t=s(e.components);return o.createElement(d.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,l=e.originalType,d=e.parentName,m=i(e,["components","mdxType","originalType","parentName"]),u=s(n),c=a,h=u["".concat(d,".").concat(c)]||u[c]||p[c]||l;return n?o.createElement(h,r(r({ref:t},m),{},{components:n})):o.createElement(h,r({ref:t},m))}));function c(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var l=n.length,r=new Array(l);r[0]=u;var i={};for(var d in t)hasOwnProperty.call(t,d)&&(i[d]=t[d]);i.originalType=e,i.mdxType="string"==typeof e?e:a,r[1]=i;for(var s=2;s<l;s++)r[s]=n[s];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}u.displayName="MDXCreateElement"},6351:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return d},metadata:function(){return s},toc:function(){return m},default:function(){return u}});var o=n(7462),a=n(3366),l=(n(7294),n(3905)),r=["components"],i={sidebar_position:7},d="Tutorial",s={unversionedId:"tutorial",id:"tutorial",title:"Tutorial",description:"This tutorial will walk through KubeDL Training, KubeDL Model and KubeDL Serving concepts.",source:"@site/docs/tutorial.md",sourceDirName:".",slug:"/tutorial",permalink:"/docs/tutorial",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"tutorialSidebar",previous:{title:"Design",permalink:"/docs/serving/design"},next:{title:"Video Demo",permalink:"/docs/video"}},m=[{value:"Install KubeDL",id:"install-kubedl",children:[],level:2},{value:"Setup docker credential",id:"setup-docker-credential",children:[],level:2},{value:"Run a job to train a model",id:"run-a-job-to-train-a-model",children:[],level:2},{value:"Inspect the job",id:"inspect-the-job",children:[],level:2},{value:"Inspect the model version",id:"inspect-the-model-version",children:[],level:2},{value:"Run the job again",id:"run-the-job-again",children:[],level:2},{value:"Inspect the model image content",id:"inspect-the-model-image-content",children:[],level:2},{value:"Serve the model",id:"serve-the-model",children:[],level:2}],p={toc:m};function u(e){var t=e.components,n=(0,a.Z)(e,r);return(0,l.kt)("wrapper",(0,o.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"tutorial"},"Tutorial"),(0,l.kt)("p",null,"This tutorial will walk through ",(0,l.kt)("a",{parentName:"p",href:"training/intro"},"KubeDL Training"),", ",(0,l.kt)("a",{parentName:"p",href:"model/intro"},"KubeDL Model")," and ",(0,l.kt)("a",{parentName:"p",href:"serving/intro"},"KubeDL Serving")," concepts."),(0,l.kt)("h2",{id:"install-kubedl"},"Install KubeDL"),(0,l.kt)("p",null,"Follow the instructions to install KubeDL. ",(0,l.kt)("a",{parentName:"p",href:"installation/install-using-yaml"},"Go \u2192")),(0,l.kt)("h2",{id:"setup-docker-credential"},"Setup docker credential"),(0,l.kt)("p",null,"The docker credential is required for KubeDL Model to store the model image.\nThis step is not needed if you don't use KubeDL model. This tutorial will use it."),(0,l.kt)("p",null,"Follow the setup instruction to setup docker credential. ",(0,l.kt)("a",{parentName:"p",href:"model/setup"},"Go \u2192")),(0,l.kt)("h2",{id:"run-a-job-to-train-a-model"},"Run a job to train a model"),(0,l.kt)("p",null,"This example trains a mnist model using distributed TensorFlow. From project root, run:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f example/tf/tf_job_mnist_modelversion.yaml\n")),(0,l.kt)("p",null,"Explanation on the ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/kubedl-io/kubedl/blob/master/example/tf/tf_job_mnist_model.yaml"},"YAML")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: "training.kubedl.io/v1alpha1"\nkind: "TFJob"\nmetadata:\n  name: "tf-distributed"\nspec:\n  cleanPodPolicy: None\n  # modelVersion defines the location where the model is stored.\n  modelVersion:\n    modelName: mymodel\n    # The dockerhub repo to push the generated image\n    imageRepo: jianhe6/mymodel\n    storage:\n      # Use hostpath, NFS is also supported.\n      localStorage:\n        # The host dir for THIS model version, each modelVersion should have its own unique parent folder, in this case, \'mymodelv1\'\n        path: /models/mymodelv1\n        # The mounted path inside the container.\n        # The training code is expected to export the model under this path, such as storing the tensorflow saved_model.\n        mountPath: /kubedl-model\n        # The node for storing the model\n        nodeName: kind-control-plane\n  tfReplicaSpecs:\n    Worker:\n      replicas: 3\n      restartPolicy: Never\n      template:\n        spec:\n          containers:\n            - name: tensorflow\n              image: kubedl/tf-mnist-estimator-api:v0.1\n              imagePullPolicy: Always\n              command:\n                - "python"\n                - "/keras_model_to_estimator.py"\n                - "/tmp/tfkeras_example/" # model checkpoint dir\n                - "/kubedl-model"         # export dir for the saved_model format\n')),(0,l.kt)("p",null,"This example stores the trained model artifacts at a hostpath, in this case ",(0,l.kt)("inlineCode",{parentName:"p"},"/models/mymodelv1"),". KubeDL will\ncreate an image that includes those model artifacts and push that to the dockerhub."),(0,l.kt)("p",null,"Notes\uff1a"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("inlineCode",{parentName:"li"},"modelVersion")," field defines where the modelVersion is stored. Currently, local hostpath and NFS are supported."),(0,l.kt)("li",{parentName:"ol"},(0,l.kt)("inlineCode",{parentName:"li"},"mountPath")," defines the path where the external storage is mounted. The training code should export the model artifacts such as the TensorFlow saved_model under this path.\nCheck the ",(0,l.kt)("a",{parentName:"li",href:"model/usage"},"documentation")," for more details")),(0,l.kt)("h2",{id:"inspect-the-job"},"Inspect the job"),(0,l.kt)("p",null,"After the job succeeded, run:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get tfjob\nNAME              STATE       AGE   TTL-AFTER-FINISHED   MAX-LIFETIME   MODEL-VERSION\ntf-distributed    Succeeded   45s                                       mv-tf-distributed\n")),(0,l.kt)("p",null,"Here, a ModelVersion named ",(0,l.kt)("inlineCode",{parentName:"p"},"mv-tf-distributed")," is created."),(0,l.kt)("p",null,"The naming convention of the ModelVersion is to prepend the ",(0,l.kt)("inlineCode",{parentName:"p"},"mv")," to the job name as such: ",(0,l.kt)("inlineCode",{parentName:"p"},"mv-<JobName>")),(0,l.kt)("h2",{id:"inspect-the-model-version"},"Inspect the model version"),(0,l.kt)("p",null,"There are ",(0,l.kt)("a",{parentName:"p",href:"model/intro"},(0,l.kt)("inlineCode",{parentName:"a"},"Model")," and ",(0,l.kt)("inlineCode",{parentName:"a"},"ModelVersion"))," . In short, ",(0,l.kt)("inlineCode",{parentName:"p"},"Model")," associates with multiple ",(0,l.kt)("inlineCode",{parentName:"p"},"ModelVersion"),"s."),(0,l.kt)("p",null,"Inspect the model version"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get mv\nNAME                   MODEL     IMAGE                    CREATED-BY       FINISH-TIME\nmv-tf-distributed      mymodel   jianhe6/mymodel:vf812c   tf-distributed   2021-07-24T00:39:02Z\n")),(0,l.kt)("p",null,"ModelVersion ",(0,l.kt)("inlineCode",{parentName:"p"},"mv-tf-distributed")," belongs to model ",(0,l.kt)("inlineCode",{parentName:"p"},"mymodel"),". The model image is ",(0,l.kt)("inlineCode",{parentName:"p"},"jianhe6/mymodel:vf812c"),".\nIt is created by job ",(0,l.kt)("inlineCode",{parentName:"p"},"tf-distributed")," at ",(0,l.kt)("inlineCode",{parentName:"p"},"2021-07-24T00:39:02Z")),(0,l.kt)("h2",{id:"run-the-job-again"},"Run the job again"),(0,l.kt)("p",null,"This time suppose we made some changes to the training code. We then change the job name to ",(0,l.kt)("inlineCode",{parentName:"p"},"tf-distributed-2")," and\nchange the ",(0,l.kt)("inlineCode",{parentName:"p"},"modelVersion.localStorage.path")," field to ",(0,l.kt)("inlineCode",{parentName:"p"},"/models/mymodelv2"),", so that the new model version will be stored at hostpath ",(0,l.kt)("inlineCode",{parentName:"p"},"/models/mymodelv2")," and not colliding with ",(0,l.kt)("inlineCode",{parentName:"p"},"/models/mymodelv1")),(0,l.kt)("p",null,"Run it again with ",(0,l.kt)("inlineCode",{parentName:"p"},"kubectl apply -f kubectl apply -f example/tf/tf_job_mnist_modelversion.yaml"),"."),(0,l.kt)("p",null,"Once the job finishes, we get below output:"),(0,l.kt)("p",null,"Two jobs, each has its own model version."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get tfjob\nNAME               STATE       AGE     TTL-AFTER-FINISHED   MAX-LIFETIME   MODEL-VERSION\ntf-distributed     Succeeded   3m57s                                       mv-tf-distributed\ntf-distributed-2   Succeeded   24s                                         mv-tf-distributed-2\n")),(0,l.kt)("p",null,"Check the model version. We get two. Each has its own generated model image."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get mv\nNAME                    MODEL     IMAGE                    CREATED-BY         FINISH-TIME\nmv-tf-distributed       mymodel   jianhe6/mymodel:vf812c   tf-distributed     2021-07-24T00:39:02Z\nmv-tf-distributed-2     mymodel   jianhe6/mymodel:vc73be   tf-distributed-2   2021-07-24T00:42:28Z\n")),(0,l.kt)("p",null,"Now, check the model,"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get model\nNAME      LATEST-VERSION          LATEST-IMAGE\nmymodel   mv-tf-distributed-2     jianhe6/mymodel:vc73be\n")),(0,l.kt)("p",null,"The model named ",(0,l.kt)("inlineCode",{parentName:"p"},"mymodel"),", has the latest model version named ",(0,l.kt)("inlineCode",{parentName:"p"}," model-version-tf-distributed-2"),", and its latest model image is ",(0,l.kt)("inlineCode",{parentName:"p"},"jianhe6/mymodel:vc73be")),(0,l.kt)("p",null,"Deleting the model will delete all its model versions."),(0,l.kt)("h2",{id:"inspect-the-model-image-content"},"Inspect the model image content"),(0,l.kt)("p",null,"Download the model image and inspect its contents. The model artifacts will be under ",(0,l.kt)("inlineCode",{parentName:"p"},"/kubedl-model"),"."),(0,l.kt)("p",null,"Run ",(0,l.kt)("inlineCode",{parentName:"p"},"docker run -it jianhe6/mymodel:vc73be /bin/sh"),", and then run ",(0,l.kt)("inlineCode",{parentName:"p"},"ls /kubedl-model")," inside the container:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"$ ls /kubedl-model/1627086141\nassets          saved_model.pb  variables\n")),(0,l.kt)("p",null,"Here, the ",(0,l.kt)("inlineCode",{parentName:"p"},"1627086141/")," parent folder is created by the training code as its version id."),(0,l.kt)("h2",{id:"serve-the-model"},"Serve the model"),(0,l.kt)("p",null,"Run an Inference workload"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f example/tf/tf_serving_modelversion.yaml\n")),(0,l.kt)("p",null,"The YAML content looks like below:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: serving.kubedl.io/v1alpha1\nkind: Inference\nmetadata:\n  name: hello-inference\nspec:\n  framework: TFServing\n  predictors:\n    - name: model-predictor\n      # The model version to be served\n      modelVersion: mv-tf-distributed-5f4c7\n      # The model path where the model is mounted inside the container,\n      # it should be the same as the tensorflow serving model_base_path\n      modelPath: /kubedl-model\n      replicas: 1\n      batching:\n        batchSize: 32\n      template:\n        spec:\n          containers:\n            - name: tensorflow\n              args:\n                - --port=9000\n                - --rest_api_port=8500\n                - --model_name=mnist\n                # This should be the same as modelPath field.\n                - --model_base_path=/kubedl-model/\n              command:\n                - /usr/bin/tensorflow_model_server\n              image: tensorflow/serving:1.11.1\n              imagePullPolicy: IfNotPresent\n              ports:\n                - containerPort: 9000\n                - containerPort: 8500\n              resources:\n                limits:\n                  cpu: 2048m\n                  memory: 2Gi\n                requests:\n                  cpu: 1024m\n                  memory: 1Gi\n")),(0,l.kt)("p",null,"Note that the ",(0,l.kt)("inlineCode",{parentName:"p"},"--model_base_path")," option of TensorFlow serving needs to be the same as the ",(0,l.kt)("inlineCode",{parentName:"p"},"modelPath")," spec in the field,\nwhich indicates where the model is mounted into the container."),(0,l.kt)("p",null,"Inspect the Inference workload state"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get inference\n")))}u.isMDXComponent=!0}}]);